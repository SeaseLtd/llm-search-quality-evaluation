from __future__ import annotations

# ------ temporary import for corpus.json bug workaround ------
import json
from pathlib import Path
from rre_tools.shared.utils import _to_string
import argparse
# -------------------------------------------------------------

from typing import List
from langchain_core.language_models import BaseChatModel
from logging import Logger, getLogger

# project imports
from rre_tools.shared.logger import setup_logging
from rre_tools.dataset_generator.llm import LLMConfig, LLMService, LLMServiceFactory
from rre_tools.shared.models import Document, Query
from rre_tools.shared.writers import WriterFactory, AbstractWriter, WriterConfig
from rre_tools.shared.search_engines import SearchEngineFactory, BaseSearchEngine
from rre_tools.shared.data_store import DataStore
from rre_tools.shared.utils import join_fields_as_text

from rre_tools.dataset_generator.models import LLMQueryResponse, LLMScoreResponse
from rre_tools.dataset_generator.config import Config

log: Logger = getLogger(__name__)


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description='Parse arguments for CLI.')

    parser.add_argument('-c', '--config', type=str,
                        help='Config file path to use for the application [default: "configs/dataset_generator/dataset_generator_config.yaml"]',
                        required=False, default="configs/dataset_generator/dataset_generator_config.yaml")

    parser.add_argument('-v', '--verbose', action='store_true',
                        help='Activate debug mode for logging [default: False]')

    return parser.parse_args()


def add_user_queries(config: Config, data_store: DataStore) -> None:
    """Loads queries from file (if exists) and adds them as Query objects."""
    if config.queries is not None:
        with config.queries.open("r", encoding="utf-8") as file:
            for line in file:
                clean_line = line.strip()
                if clean_line:
                    data_store.add_query(clean_line)
            log.info(f"Added user-defined queries from file={config.queries}")


def generate_and_add_queries(config: Config, data_store: DataStore, llm_service: LLMService,
                             search_engine: BaseSearchEngine) -> None:
    """Retrieve docs and generate queries with LLM Service. Adds docs, queries and ratings to the datastore."""
    docs_to_generate_queries: List[Document] = search_engine.fetch_for_query_generation(
        documents_filter=config.documents_filter,
        number_of_docs=config.number_of_docs,
        doc_fields=config.doc_fields
    )

    for doc in docs_to_generate_queries:
        doc.is_used_to_generate_queries = True
        data_store.add_document(doc)

    remaining = max(0, config.num_queries_needed - len(data_store.get_queries()))
    if remaining == 0:
        return

    num_queries_per_doc: int = int((remaining // max(1, config.number_of_docs)) + 1)  # always greater or equal to 1
    log.debug(f"Number of documents retrieved for generation: {len(docs_to_generate_queries)}")
    log.debug(f"Pending queries to generate: {remaining}")
    log.debug(f"Number of queries per document: {num_queries_per_doc}")

    for doc in docs_to_generate_queries:
        query_response: LLMQueryResponse = llm_service.generate_queries(doc, num_queries_per_doc,
                                                                        config.max_query_terms)
        for query_ in query_response.get_queries():
            if len(data_store.get_queries()) >= config.num_queries_needed:
                return
            query_obj: Query = data_store.add_query(query_)
            data_store.create_rating_score(
                query_obj.id, doc.id, max(config.relevance_label_set),
                "Default max rating is assigned because the query is generated by the document"
            )


def add_cartesian_product_scores(config: Config, data_store: DataStore, llm_service: LLMService) -> None:
    """Complete the (query, doc) matrix with LLM scores."""
    log.debug("Cartesian product is enabled, so adding cartesian product scores")
    for query_obj in data_store.get_queries():
        for doc_obj in data_store.get_cartesian_prod_docs():
            if not data_store.has_rating_score(query_obj.id, doc_obj.id):
                score_resp: LLMScoreResponse = llm_service.generate_score(
                    doc_obj, query_obj.text, config.relevance_scale, config.save_llm_explanation
                )
                data_store.create_rating_score(
                    query_obj.id, doc_obj.id, score_resp.get_score(),
                    score_resp.explanation if config.save_llm_explanation else None
                )


def expand_docset_with_search_engine_top_k(config: Config, data_store: DataStore,
                                           llm_service: LLMService, search_engine: BaseSearchEngine) -> None:
    """Retrieve docs for each query and score the (q, doc) pairs."""
    if config.query_template is not None:
        log.debug(f"Searching for documents with query template in {config.query_template}")
        for query_obj in data_store.get_queries():
            docs_eval: List[Document] = search_engine.fetch_for_evaluation(
                keyword=query_obj.text, query_template=config.query_template, doc_fields=config.doc_fields
            )
            for doc_obj in docs_eval:
                data_store.add_document(doc_obj)
                if not data_store.has_rating_score(query_obj.id, doc_obj.id):
                    score_resp: LLMScoreResponse = llm_service.generate_score(
                        doc_obj, query_obj.text, config.relevance_scale, config.save_llm_explanation
                    )
                    data_store.create_rating_score(
                        query_obj.id, doc_obj.id, score_resp.score,
                        score_resp.explanation if config.save_llm_explanation else None
                    )
    else:
        log.warning("Query template not found. Skipping retrieval.")


def main() -> None:
    # configuration and logger definition
    args = parse_args()
    config: Config = Config.load(args.config)
    writer_config: WriterConfig = config.build_writer_config()
    setup_logging(args.verbose)

    # setup
    data_store: DataStore = DataStore(
        autosave_every_n_updates=config.datastore_autosave_every_n_updates
    )
    search_engine: BaseSearchEngine = SearchEngineFactory.build(
        search_engine_type=config.search_engine_type,
        endpoint=config.search_engine_collection_endpoint
    )
    llm: BaseChatModel = LLMServiceFactory.build(LLMConfig.load(config.llm_configuration_file))
    service: LLMService = LLMService(chat_model=llm)
    writer: AbstractWriter = WriterFactory.build(writer_config)

    # load user queries
    add_user_queries(config, data_store)

    # generate more queries with LLM service if needed
    generate_and_add_queries(config, data_store, service, search_engine)

    # score initial docset
    if config.enable_cartesian_product:
        add_cartesian_product_scores(config, data_store, service)

    # expand the docset with search engine topK (adding direct ratings)
    expand_docset_with_search_engine_top_k(config, data_store, service, search_engine)

    # write results
    output_destination = config.output_destination
    log.info(f"Synthetic Dataset has been generated in: {output_destination}")
    data_store.save()
    writer.write(output_destination, data_store)

    # save explanation  - forced to extract value before invoking export_all_records_with_explanation (mypy)
    if config.save_llm_explanation:
        if llm_explanation_path := config.llm_explanation_destination:
            data_store.export_all_records_with_explanation(llm_explanation_path)
            log.info(f"Dataset with LLM explanation is saved into: {llm_explanation_path}")

    # TODO:
    #  work on a better solution, instead of overwriting the corpus.json file, and maybe modify the MtebWriter with the
    #  fetch from the search engine
    if config.output_format == "mteb":
        # copy pasted from MtebWriter
        corpus_path = Path(output_destination) / "corpus.jsonl"
        corpus_path.unlink(missing_ok=True)
        with corpus_path.open("a", encoding="utf-8") as file:
            for doc in search_engine.fetch_all(doc_fields=config.doc_fields):
                doc_id = str(doc.id)
                fields = doc.fields
                title = _to_string(fields.get("title"))
                text = join_fields_as_text(fields=fields, exclude={'id', 'title'})

                row = {"id": doc_id, "title": title, "text": text}
                file.write(json.dumps(row, ensure_ascii=False) + "\n")


if __name__ == "__main__":
    main()
