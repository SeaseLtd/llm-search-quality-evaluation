from __future__ import annotations
from typing import List
from langchain_core.language_models import BaseChatModel
from logging import Logger, getLogger, DEBUG, INFO

# project imports
from src.config import Config
from src.utils import parse_args
from src.logger import configure_logging
from src.llm import LLMConfig, LLMService, LLMServiceFactory
from src.model import Document, Query,  LLMQueryResponse, LLMScoreResponse
from src.writers import WriterFactory, AbstractWriter
from src.search_engine import SearchEngineFactory, BaseSearchEngine
from src.data_store import DataStore



def get_and_setup_logging(verbose: bool = False) -> Logger:
    if verbose:
        configure_logging(DEBUG)
    else:
        configure_logging(INFO)
    return getLogger(__name__)


def add_user_queries(config: Config, data_store: DataStore) -> None:
    """Loads queries from file (if exists) and adds them as Query objects."""
    if config.queries is not None:
        with open(config.queries, "r", encoding="utf-8") as file:
            for line in file:
                clean_line = line.strip()
                if clean_line:
                    data_store.add_query(clean_line)


def generate_and_add_queries(config: Config, data_store: DataStore, llm_service: LLMService, search_engine: BaseSearchEngine) -> None:
    """Retrieve docs and generate queries with LLM Service. Adds docs, queries and ratings to the datastore."""
    docs_to_generate_queries: List[Document] = search_engine.fetch_for_query_generation(
        documents_filter=config.documents_filter,
        doc_number=config.doc_number,
        doc_fields=config.doc_fields
    )
    remaining = max(0, config.num_queries_needed - len(data_store.get_queries()))
    num_queries_per_doc: int = int((remaining // max(1, config.doc_number)) * 1.5)
    log.debug(f"Number of documents retrieved for generation: {len(docs_to_generate_queries)}")
    log.debug(f"Pending queries to generate: {remaining}")
    log.debug(f"Number of queries per document: {num_queries_per_doc}")

    for doc in docs_to_generate_queries:
        data_store.add_document(doc)

        query_response: LLMQueryResponse = llm_service.generate_queries(doc, num_queries_per_doc)
        for query_ in query_response.get_queries():
            if len(data_store.get_queries()) >= config.num_queries_needed:
                return
            query_obj: Query = data_store.add_query(query_)
            data_store.create_rating_score(
                query_obj.id, doc.id, max(config.relevance_label_set),
                "Default max rating is assigned because the query is generated by the document"
            )


def add_cartesian_product_scores(config: Config, data_store: DataStore, llm_service: LLMService) -> None:
    """Complete the (query, doc) matrix with LLM scores."""
    for query_obj in data_store.get_queries():
        for doc_obj in data_store.get_documents():
            if not data_store.has_rating_score(query_obj.id, doc_obj.id):
                score_resp: LLMScoreResponse = llm_service.generate_score(
                    doc_obj, query_obj.text, config.relevance_scale, config.save_llm_explanation
                )
                data_store.create_rating_score(
                    query_obj.id, doc_obj.id, score_resp.get_score(),
                    score_resp.explanation if config.save_llm_explanation else None
                )


def expand_docset_with_search_engine_top_k(config: Config, data_store: DataStore,
                                 llm_service: LLMService, search_engine: BaseSearchEngine) -> None:
    """Retrieve docs for each query and score the (q, doc) pairs."""
    for query_obj in data_store.get_queries():
        docs_eval: List[Document] = search_engine.fetch_for_evaluation(
            keyword=query_obj.text, query_template=config.query_template, doc_fields=config.doc_fields
        )
        for doc_obj in docs_eval:
            data_store.add_document(doc_obj)
            if not data_store.has_rating_score(query_obj.id, doc_obj.id):
                score_resp: LLMScoreResponse = llm_service.generate_score(
                    doc_obj, query_obj.text, config.relevance_scale, config.save_llm_explanation
                )
                data_store.create_rating_score(
                    query_obj.id, doc_obj.id, score_resp.score,
                    score_resp.explanation if config.save_llm_explanation else None
                )


if __name__ == "__main__":
    # configuration and logger definition
    args = parse_args()
    config: Config = Config.load(args.config_file)
    log: Logger = get_and_setup_logging(args.verbose)

    # setup
    data_store: DataStore = DataStore()
    search_engine: BaseSearchEngine = SearchEngineFactory.build(
        search_engine_type=config.search_engine_type,
        endpoint=config.search_engine_collection_endpoint
    )
    llm: BaseChatModel = LLMServiceFactory.build(LLMConfig.load(config.llm_configuration_file))
    service: LLMService = LLMService(chat_model=llm)
    writer: AbstractWriter = WriterFactory.build(config.output_format) 
    
    # load user queries
    add_user_queries(config, data_store)

    # generate more queries with LLM service if needed
    generate_and_add_queries(config, data_store, service, search_engine)

    # score initial docset
    add_cartesian_product_scores(config, data_store, service)

    # expand the docset with search engine topK (adding direct ratings)
    expand_docset_with_search_engine_top_k(config, data_store, service, search_engine)

    # write results
    output_destination = config.output_destination
    writer.write(output_destination, data_store)
    log.info(f"Synthetic Dataset has been generated in: {output_destination}")

    # save explanation  - forced to extract value before invoking export_all_records_with_explanation (mypy)
    if llm_explanation_path := config.llm_explanation_destination:
        data_store.export_all_records_with_explanation(llm_explanation_path)
        log.info(f"Dataset with LLM explanation is saved into: {llm_explanation_path}")
