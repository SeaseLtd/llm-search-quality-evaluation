# (Optional) Template string given by the user to evaluate the search system using relevance judgements generated by the
# dataset-generator module
# Default: "q=#$query##"
# Example (solr):
query_template: "q=#$query##&df=description&rows=2&q.op=OR&wt=json"

# The type of search engine to use
# Accepted values: solr, elasticsearch, opensearch, vespa
search_engine_type: "solr"

# Endpoint URL of the search engine collection or index
# opensearch: http://localhost:9200/testcore
search_engine_collection_endpoint: "http://localhost:8983/solr/testcore/"

# (Optional) Filter query to restrict the set of documents used to generate queries.
# If a field has more than one value, the documents retrieved have at least one of the values in the field (OR-
# -like). If there are more than one fields, the documents are filtered for both fields (AND-like)
# Default: all documents are considered and no filter is applied
#documents_filter:
#  - genre:
#      - "horror"
#      - "fantasy"
#  - type:
#      - "book"

# Maximum number of documents to retrieve from the search engine to generate queries
doc_number: 2

# List of fields from documents used to generate queries and relevance scoring
# These should match fields available in the search engine schema
# Must be at least one field
doc_fields:
  - "title"
  - "description"

# (Optional) File containing predefined queries to use instead of or alongside generated ones
# If available, must be in a txt format.
#queries: "queries.txt"

# (Optional) Whether to generate queries from documents
# Default: true; set to false to disable query generation from documents
generate_queries_from_documents: true

# Total number of queries to generate (includes predefined queries, if any)
num_queries_needed: 4

# Relevance scale used for scoring document relevance
# Accepted values:
#   - binary: 0 (not relevant), 1 (relevant)
#   - graded: 0 (not relevant), 1 (maybe ok), 2 (thatâ€™s my result)
relevance_scale: "graded"

# Path to the LLM configuration file (e.g., LangChain setup)
llm_configuration_file: "llm_config.yaml"

# Output format for the generated dataset
# Accepted values: quepid, rre
output_format: "quepid"

# Path where the output dataset will be saved
output_destination: "output/generated_dataset.csv"

# (Optional) Whether to save LLM rating score explanation to file
# Default: false; set to true to save LLM rating explanation
save_llm_explanation: true

# (Optional**) File path where it contains <query, doc_id, rating, explanation> records.
# (**) When save_llm_explanation is set to True, this param needs to be present
llm_explanation_destination: "output/rating_explanation.json"
