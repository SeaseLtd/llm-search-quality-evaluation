# (Optional) Template string for generated/given queries used to search documents and build relevance
# judgemenets, with a placeholder for inserting keywords
# Example (Solr):
query_template: "q=#$query##&fq=genre:horror&wt=json"

# The type of search engine to use
# Accepted values: Solr, Elasticsearch, Opensearch, Vespa
search_engine_type: "solr"

# Endpoint URL of the search engine collection or index
search_engine_collection_endpoint: "http://localhost:8983/solr/mycore"

# (Optional) Filter query to restrict the set of documents used to generate queries. Top k retrieved documents
# are used, with k computed accordingly looking at the params: docNumber, totalNumQueriesToGenerate
# If a field has more than one value, the documents retrieved have at least one of the values in the field (OR-
# -like). If there are more than one fields, the documents are filtered for both fields (AND-like)
# Default: all documents are considered and no filter is applied
documents_filter:
  - genre:
      - "horror"
      - "fantasy"
  - type:
      - "book"

# Maximum number of documents to retrieve from the search engine to generate queries
doc_number: 100

# List of fields from documents used to generate context and relevance scoring
# These should match fields available in the search engine schema
# Must be at least one field
doc_fields:
  - "title"
  - "description"

# (Optional) File containing predefined queries to use instead of or alongside generated ones
# If available, must be in a JSON format.
queries: "queries.txt"

# (Optional) Whether to generate queries from documents
# Default: true; set to false to disable query generation from documents
generate_queries_from_documents: true

# Total number of queries to generate (includes predefined queries, if any)
total_num_queries_to_generate: 10

# Relevance scale used for scoring document relevance
# Accepted values:
#   - binary: 0 (not relevant), 1 (relevant)
#   - graded: 0 (not relevant), 1 (may be ok), 2 (thatâ€™s my result)
relevance_scale: "graded"

# Path to the LLM configuration file (e.g., LangChain setup)
llm_configuration_file: "llm_config.yaml"

# Output format for the generated dataset
# Accepted values: Quepid, RRE
output_format: "Quepid"

# Path where the output dataset will be saved
output_destination: "output/generated_dataset.json"

# Whether to generate an additional file with query, document, score, and explanation
# Accepted values: true, false
output_explanation: true
